{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_afterFineTuning_WithoutAug_all_94_58_CLEANED.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90a8sE_abWS"
      },
      "source": [
        "# Project - Cdiscount Image Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQSDw9jO3VP9"
      },
      "source": [
        "# Data ingestion\n",
        "The primary training set is a 57GB bson file, having ~15 Million images (180x180 images in Base64 format) of ~7.06 Million products. We have imported the dataset into a MongoDB instance on a VPS, so we were able to query among the records.\n",
        "We have chosen 100 categories, which overally consist of ~246K images of ~110K products.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NunsFV4jxHc"
      },
      "source": [
        "## Dataset preparation\n",
        "\n",
        "First we need to ensure that the \"gdown\" library is installed and accessible in the environment and download the train_medium data from Google Drive,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVPqZO_a0mc5",
        "outputId": "9221f1a1-eacc-4b29-ff9d-bb2f0b435772"
      },
      "source": [
        "! pip install gdown && gdown --id 1F6Xf4yiYxeFEN6qhrL3YBNs0Vhx0bXJ1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F6Xf4yiYxeFEN6qhrL3YBNs0Vhx0bXJ1\n",
            "To: /content/train_shuffled_100cat.csv\n",
            "1.62GB [00:09, 178MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpivBrTeja1s"
      },
      "source": [
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds # tensorflow datasets - pip install tensorflow-datasets\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import transform # pip install scikit-image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8ZkDULUNaOV"
      },
      "source": [
        "## Note for the team\n",
        "Since the original dataset is pretty large, I've created a subset file containing ~250K photos in 100 categories, but it is still so large that it may not fit into the memory, so I've used the below parameters to load a fitable subset accordingly, please read the comments of each variable careflully, and do not change the loading code please, just set the values of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOFXWL41NaOW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "import base64\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni7OSQvyNaOW"
      },
      "source": [
        "Run the cell below if you have a gpu that you want to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IVK6e4JNaOW"
      },
      "source": [
        "#physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "#if len(physical_devices):\n",
        "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "#print(physical_devices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gg39dI6f-E"
      },
      "source": [
        "## 1. Load your data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM2fnrxPNaOX"
      },
      "source": [
        "CONVERT_TO_NP_ARRAY= False   # Wether convert the Base64 string to (180,180,3) arrays or keep the Base64 string.\n",
        "\n",
        "REPLACE_BASE64_SPECIAL_CHARS = True # If you have the base64 decoding layer in your model, \n",
        "                                    # you need to set this to True to replace the two special characters in Base64 that is incompatible with tf image reader\n",
        "\n",
        "LOADING_MODE = \"all\" \n",
        "                             # num_records: Loads the first NUM_RECORDS in the dataset and calculates NUM_CATEGORIES dynamically\n",
        "                             # num_categories: Loads first NUM_CATEGORIES and calculates NUM_RECORDS dynamically\n",
        "                             # all: Loads all the 250K images, ignores all parameters below\n",
        "                            \n",
        "    \n",
        "NUM_RECORDS = 3000           # Only used when the mode is set to num_records\n",
        "NUM_CATEGORIES = 100           # Only used when the mode is set to num_category\n",
        "MAX_RECORDS_PER_CATEGORY = 700 # if not zero, will ensure that there is no more per category in the dataframe, won't work when mode is set to all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em6RkMvGNaOX"
      },
      "source": [
        "def get_array_from_base64(img, shape=(180,180,3)):\n",
        "    print(img)\n",
        "    return np.array(\n",
        "        Image.open(\n",
        "            io.BytesIO(\n",
        "                base64.b64decode(\n",
        "                    img.replace('_', '/').replace('-', '+') if REPLACE_BASE64_SPECIAL_CHARS else img\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "    ).reshape(shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t1RKH_QNaOX",
        "outputId": "13d7855e-f4e2-4526-c9af-ecb7ca25fd29"
      },
      "source": [
        "FILE_NAME= \"train_shuffled_100cat.csv\"\n",
        "header = 3\n",
        "\n",
        "df = pd.read_csv(FILE_NAME, header=3, nrows=0)\n",
        "\n",
        "if LOADING_MODE == \"all\":\n",
        "    df = pd.read_csv(FILE_NAME, header=header)\n",
        "\n",
        "if LOADING_MODE == \"num_records\":\n",
        "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=min(NUM_RECORDS, 100))\n",
        "    for chunk in reader:\n",
        "        df = df.append(chunk, ignore_index=True)\n",
        "        if MAX_RECORDS_PER_CATEGORY:\n",
        "            for cat in df[\"category_id\"].unique():\n",
        "                if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
        "                    removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
        "                    df = df.drop(removed_rows.index)\n",
        "        if df.shape[0]>=NUM_RECORDS:\n",
        "            df = df.head(NUM_RECORDS)\n",
        "            break\n",
        "    \n",
        "elif LOADING_MODE == \"num_categories\": \n",
        "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=100)\n",
        "    for chunk in reader:\n",
        "        df = df.append(chunk, ignore_index=True)\n",
        "        if df[\"category_id\"].nunique() > NUM_CATEGORIES:\n",
        "            break\n",
        "    if MAX_RECORDS_PER_CATEGORY:\n",
        "        for cat in df[\"category_id\"].unique():\n",
        "            if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
        "                removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
        "                df = df.drop(removed_rows.index)\n",
        "\n",
        "    cat_removed = df[\"category_id\"].unique()[NUM_CATEGORIES:]\n",
        "    df = df.loc[~df['category_id'].isin(cat_removed)]\n",
        "    NUM_RECORDS= df.shape[0]\n",
        "\n",
        "if CONVERT_TO_NP_ARRAY:        \n",
        "    df[\"image\"] = df[\"image\"].apply(\n",
        "                    lambda x: get_array_from_base64(x)\n",
        "                )\n",
        "if REPLACE_BASE64_SPECIAL_CHARS:\n",
        "    df[\"image\"] = df[\"image\"].apply(\n",
        "                    lambda x: x.replace('/', '_').replace('+', '-')\n",
        "                )\n",
        "    \n",
        "NUM_CATEGORIES = df['category_id'].nunique()\n",
        "\n",
        "categories = df['category_id'].unique()\n",
        "categories.sort()\n",
        "category_id_map = {k: v for v, k in enumerate(categories)}\n",
        "df[\"class\"] = df[\"category_id\"].apply(lambda x: category_id_map[x])\n",
        "\n",
        "print(\"Num records:\", NUM_RECORDS)\n",
        "print(\"Num categories:\", NUM_CATEGORIES)\n",
        "print(\"Training df shape:\", df.shape)\n",
        "print(\"Mem used by images:\", int(sum(df[\"image\"].apply(lambda x: x.nbytes if type(x)!=str else len(x))/10 ** 6)), \"MB\")\n",
        "# print(len(df.at[0, \"image\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num records: 3000\n",
            "Num categories: 99\n",
            "Training df shape: (246261, 4)\n",
            "Mem used by images: 1613 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qa5Rle5NaOY"
      },
      "source": [
        "df = df.sample(frac=1,random_state = 123)\n",
        "df.reset_index(drop=True, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94KecGabAoB",
        "outputId": "f43bd2a0-8bff-43fe-c0ec-9cc80fe39319"
      },
      "source": [
        "X_dev = np.stack(df[\"image\"]) if CONVERT_TO_NP_ARRAY else np.array(df[\"image\"])\n",
        "Y_dev = np.array(df[\"class\"])\n",
        "print(X_dev.shape,Y_dev.shape, Y_dev[-10:])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(246261,) (246261,) [18 45 86 50 70 44 62 15 18 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0KQWQTcbjUx"
      },
      "source": [
        "## 2. Explore your data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSVj9Xm2_Qsl"
      },
      "source": [
        "Showing 10 samples from dev set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xxMSDDkB6f-G",
        "scrolled": true,
        "outputId": "e995679c-7060-48d9-cc39-18870a9e88d9"
      },
      "source": [
        "plt.figure(figsize=(12, 6), dpi=1000)\n",
        "indexes = np.arange(len(X_dev))\n",
        "np.random.shuffle(indexes)\n",
        "if CONVERT_TO_NP_ARRAY:\n",
        "    for idx in range(10):\n",
        "      plt.subplot(2, 5, idx + 1)\n",
        "      plt.imshow(X_dev[indexes[idx]])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 12000x6000 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hExX4dqr_XNZ"
      },
      "source": [
        "#Splitting dev set into train/val set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv6OSGIO_bfG",
        "outputId": "6ab9a080-6f69-4529-885e-2941635a9671"
      },
      "source": [
        "# Splitting point of train/val set\n",
        "num_train = int(len(X_dev) * .75) \n",
        "num_val = int(len(X_dev) * .2)\n",
        "num_test = len(X_dev) - num_train - num_val\n",
        "\n",
        "X_train = X_dev[indexes[:num_train]]\n",
        "Y_train = Y_dev[indexes[:num_train]]\n",
        "\n",
        "X_val = X_dev[indexes[num_train:-num_test]]\n",
        "Y_val = Y_dev[indexes[num_train:-num_test]]\n",
        "\n",
        "X_test = X_dev[indexes[-num_test:]]\n",
        "Y_test = Y_dev[indexes[-num_test:]]\n",
        "\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_val:\", X_val.shape)\n",
        "print(\"X_test:\", X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train: (184695,)\n",
            "X_val: (49252,)\n",
            "X_test: (12314,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfQzijUy6f-J"
      },
      "source": [
        "## 3. Data scaling and Data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n4rt2mGNaOa"
      },
      "source": [
        "### Preprocess_and_decode:\n",
        "It is a function which applies to each image input to the model, it first decodes the JPEG base64 encoded image to a tensor, then it scales it based on the sample's min, max, mean and std.\n",
        "Since we are normalizing our data per sample, our normalization is row-wise and a column-wise normalization is not yet an option. We can do this by preprocessing our dataset by batching it if that seemed necessary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCdL608yNaOb"
      },
      "source": [
        "def preprocess_and_decode(img_str, new_shape=(180,180), scaling_mode = 2 ): #scaling_mode= 0: disabled, 1: min-max normalization, 2: standardization\n",
        "    img = tf.io.decode_base64(img_str)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, new_shape, method=tf.image.ResizeMethod.BILINEAR)\n",
        "   \n",
        "    if scaling_mode == 1: \n",
        "      img_min = tf.math.reduce_min(img, axis=None, keepdims=False, name=None)\n",
        "      img_max = tf.math.reduce_max(img, axis=None, keepdims=False, name=None)\n",
        "      img = ( img - img_min ) / (img_max - img_min)\n",
        "      \n",
        "    elif scaling_mode == 2:\n",
        "      img_mean = tf.math.reduce_mean(img, axis=None, keepdims=False, name=None)\n",
        "      img_std = tf.math.reduce_std(img, axis=None, keepdims=False, name=None)\n",
        "      img = ( img - img_mean ) / img_std\n",
        "      \n",
        "    return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUdBkorNaOb"
      },
      "source": [
        "### About data augmentation\n",
        "Since our dataset consist of 2-4 image per product, and we have over 2K images per category, data augmentation seems unnecessary.\n",
        "I still have not found a way to do data augmentation on Base64 strings(remember I moved to Base64-> tensor decoding inside the model itself to enable us to do batch training on the whole dataset), but if we come to a need for it, there is definitly a way to do that!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibIuCJatMuDE"
      },
      "source": [
        "## 4. Define your callbacks (save your model, patience, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLM8gBM-p24a"
      },
      "source": [
        "# Defining callbacks for the model\n",
        "model_name = \"vgg_project.h5\"\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
        "\n",
        "monitor = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\\\n",
        "                                             verbose=0,save_best_only=True,\\\n",
        "                                             save_weights_only=True,\\\n",
        "                                             mode='min')\n",
        "# Learning rate schedule\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch%4 == 0 and epoch!= 0:\n",
        "        lr = lr/2\n",
        "    return lr\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3E9YADeNtGF"
      },
      "source": [
        "## 6. Transfer Learning\n",
        "\n",
        "6.1 Choose and load your pretrained model without the top (i.e., the prediction part, usually the fully connected layers)\n",
        "\n",
        "6.2. Freeze the layers (i.e., make them non-trainable) of your pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysy_-qIgNvoc",
        "outputId": "7c4f4347-dc04-4224-9523-9d78e64fc8a9"
      },
      "source": [
        "# Dimensions we will resize the images\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "k = NUM_CATEGORIES # k = 99\n",
        "input64 = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "img_tensor = tf.keras.layers.Lambda(\n",
        "    lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype=\"float32\"))(input64)\n",
        "# Importing VGG16 structure for transfer learning\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    include_top=False) \n",
        "base_model.trainable = False # so the base model does not train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H0RXNaJNx6d"
      },
      "source": [
        "6.3. Add a top (i.e., the prediction layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjBs5BDpN0gB",
        "outputId": "7d306b41-2f4a-4ba2-dc06-2e540fcbd134"
      },
      "source": [
        "x1 = base_model(img_tensor, training=False)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(k,activation = 'softmax')(x2) # k = 99 categories\n",
        "model = tf.keras.Model(inputs = input64, outputs = out)\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 5, 5, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 99)                1267299   \n",
            "=================================================================\n",
            "Total params: 15,981,987\n",
            "Trainable params: 1,267,299\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwIJHQdu3o0Z"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRP94UYAN4Bs",
        "outputId": "af915712-c2ac-48aa-d47b-7ff895d8b6d7"
      },
      "source": [
        "# Training the model \n",
        "model.fit(X_train, Y_train,epochs = 20, \\\n",
        "          verbose = 1, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val,Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "5772/5772 [==============================] - 813s 135ms/step - loss: 1.1730 - accuracy: 0.7348 - val_loss: 0.6032 - val_accuracy: 0.8521\n",
            "Epoch 2/20\n",
            "5772/5772 [==============================] - 767s 133ms/step - loss: 0.4435 - accuracy: 0.8870 - val_loss: 0.5243 - val_accuracy: 0.8745\n",
            "Epoch 3/20\n",
            "5772/5772 [==============================] - 766s 133ms/step - loss: 0.3039 - accuracy: 0.9223 - val_loss: 0.5036 - val_accuracy: 0.8806\n",
            "Epoch 4/20\n",
            "5772/5772 [==============================] - 766s 133ms/step - loss: 0.2313 - accuracy: 0.9409 - val_loss: 0.4908 - val_accuracy: 0.8859\n",
            "Epoch 5/20\n",
            "5772/5772 [==============================] - 767s 133ms/step - loss: 0.1747 - accuracy: 0.9587 - val_loss: 0.4834 - val_accuracy: 0.8891\n",
            "Epoch 6/20\n",
            "5772/5772 [==============================] - 767s 133ms/step - loss: 0.1494 - accuracy: 0.9662 - val_loss: 0.4887 - val_accuracy: 0.8900\n",
            "Epoch 7/20\n",
            "5772/5772 [==============================] - 766s 133ms/step - loss: 0.1325 - accuracy: 0.9705 - val_loss: 0.4891 - val_accuracy: 0.8913\n",
            "Epoch 8/20\n",
            "5772/5772 [==============================] - 766s 133ms/step - loss: 0.1194 - accuracy: 0.9735 - val_loss: 0.4912 - val_accuracy: 0.8918\n",
            "Epoch 9/20\n",
            "5772/5772 [==============================] - 765s 133ms/step - loss: 0.1032 - accuracy: 0.9788 - val_loss: 0.4938 - val_accuracy: 0.8932\n",
            "Epoch 10/20\n",
            "5772/5772 [==============================] - 764s 132ms/step - loss: 0.0989 - accuracy: 0.9794 - val_loss: 0.4971 - val_accuracy: 0.8933\n",
            "Epoch 11/20\n",
            "5772/5772 [==============================] - 764s 132ms/step - loss: 0.0933 - accuracy: 0.9810 - val_loss: 0.4999 - val_accuracy: 0.8930\n",
            "Epoch 12/20\n",
            "5772/5772 [==============================] - 766s 133ms/step - loss: 0.0892 - accuracy: 0.9822 - val_loss: 0.5021 - val_accuracy: 0.8935\n",
            "Epoch 13/20\n",
            "5772/5772 [==============================] - 765s 133ms/step - loss: 0.0824 - accuracy: 0.9840 - val_loss: 0.5049 - val_accuracy: 0.8933\n",
            "Epoch 14/20\n",
            "5772/5772 [==============================] - 765s 133ms/step - loss: 0.0797 - accuracy: 0.9847 - val_loss: 0.5088 - val_accuracy: 0.8936\n",
            "Epoch 15/20\n",
            "5772/5772 [==============================] - 768s 133ms/step - loss: 0.0795 - accuracy: 0.9842 - val_loss: 0.5089 - val_accuracy: 0.8934\n",
            "Epoch 16/20\n",
            "5772/5772 [==============================] - 769s 133ms/step - loss: 0.0759 - accuracy: 0.9852 - val_loss: 0.5102 - val_accuracy: 0.8932\n",
            "Epoch 17/20\n",
            "5772/5772 [==============================] - 768s 133ms/step - loss: 0.0737 - accuracy: 0.9860 - val_loss: 0.5118 - val_accuracy: 0.8934\n",
            "Epoch 18/20\n",
            "5772/5772 [==============================] - 771s 134ms/step - loss: 0.0730 - accuracy: 0.9858 - val_loss: 0.5140 - val_accuracy: 0.8934\n",
            "Epoch 19/20\n",
            "5772/5772 [==============================] - 769s 133ms/step - loss: 0.0735 - accuracy: 0.9860 - val_loss: 0.5141 - val_accuracy: 0.8933\n",
            "Epoch 20/20\n",
            "5772/5772 [==============================] - 769s 133ms/step - loss: 0.0714 - accuracy: 0.9864 - val_loss: 0.5158 - val_accuracy: 0.8934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d2206b850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSaqHUanY-RB"
      },
      "source": [
        "6.4. Testing before fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afeRwubQZAap",
        "outputId": "a086ce17-9bde-499c-e781-32fe99410cbc"
      },
      "source": [
        "# Testing the trained model\n",
        "model.load_weights(model_name)\n",
        "metrics = model.evaluate(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "385/385 [==============================] - 80s 114ms/step - loss: 0.2389 - accuracy: 0.9433\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNAM0SGY7wx"
      },
      "source": [
        "6.5. Fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGZNOieaN42k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3528aa42-ede4-4a91-80b3-41fec26e71a6"
      },
      "source": [
        "# Making the base model trainable for fine tunning\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(img_height, img_width, 3),\n",
        "    include_top=False) \n",
        "base_model.trainable = True\n",
        "input64 = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
        "img_tensor = tf.keras.layers.Lambda(\n",
        "    lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype=\"float32\"))(input64)\n",
        "\n",
        "x1 = base_model(img_tensor, training=True)\n",
        "x2 = tf.keras.layers.Flatten()(x1)\n",
        "out = tf.keras.layers.Dense(k,activation = 'softmax')(x2)\n",
        "model = tf.keras.Model(input64, out)\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 180, 180, 3)       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 5, 5, 512)         14714688  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 99)                1267299   \n",
            "=================================================================\n",
            "Total params: 15,981,987\n",
            "Trainable params: 15,981,987\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjAvAw57ONV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4718794-232a-4081-c816-50128d15ae98"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-6),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "# Loading the saved weights to fine tune the model\n",
        "model.load_weights(model_name)\n",
        "# Fine tuning the model\n",
        "model.fit(X_train, Y_train,batch_size = 32, epochs = 7, \\\n",
        "          verbose = 1, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val, Y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "5772/5772 [==============================] - 1831s 317ms/step - loss: 2.9883 - accuracy: 0.2642 - val_loss: nan - val_accuracy: 0.1059\n",
            "Epoch 2/7\n",
            "5772/5772 [==============================] - 1802s 312ms/step - loss: 3.4283 - accuracy: 0.1356 - val_loss: nan - val_accuracy: 0.1372\n",
            "Epoch 3/7\n",
            "5772/5772 [==============================] - 1800s 312ms/step - loss: 3.4193 - accuracy: 0.1371 - val_loss: nan - val_accuracy: 0.1372\n",
            "Epoch 4/7\n",
            "5772/5772 [==============================] - 1796s 311ms/step - loss: 3.4226 - accuracy: 0.1377 - val_loss: nan - val_accuracy: 0.1372\n",
            "Epoch 5/7\n",
            "5772/5772 [==============================] - 1791s 310ms/step - loss: 3.4152 - accuracy: 0.1367 - val_loss: nan - val_accuracy: 0.1372\n",
            "Epoch 6/7\n",
            "5772/5772 [==============================] - 1790s 310ms/step - loss: 3.4197 - accuracy: 0.1391 - val_loss: nan - val_accuracy: 0.1372\n",
            "Epoch 7/7\n",
            "5772/5772 [==============================] - 1793s 311ms/step - loss: 3.4197 - accuracy: 0.1383 - val_loss: nan - val_accuracy: 0.1372\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f36fdd65f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1dlyCsu6f-U"
      },
      "source": [
        "## 8. Test your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfhhkxiNOSNN",
        "outputId": "f6bf515a-16ce-4401-aa8e-7c3549b14e29"
      },
      "source": [
        "# Testing the model after fine tunning\n",
        "model.load_weights(model_name)\n",
        "metrics = model.evaluate(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "385/385 [==============================] - 42s 109ms/step - loss: 0.2367 - accuracy: 0.9458\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}